{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c9c97e-20bb-45d2-96d3-100437094f57",
   "metadata": {},
   "source": [
    "### Training a Custom Deep Learning Model © Oxcitas Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc94d-fce5-4f65-91bd-f9c7719c6cf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6535b7-6d97-4691-b5bd-b752740a1e5b",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f3495-05cb-4d02-b75e-066459775d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c142e6-16d3-47d8-855e-970acc319f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Suppress TensorFlow INFO and WARNING messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f58306e-1edb-45a8-b7bd-52deb4df83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory of `src` to the Python path\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1aad589-f9e3-4377-a8b5-01798a54585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports (placeholders for demonstration)\n",
    "import src.constants as C\n",
    "from src.data import Dataset\n",
    "from src.models import Custom3DCNN, SFCN, resnet18\n",
    "from src.utils import AWDLoss, KLDivLoss, augment, scheduler, discretize_ages, save_subject_ids, load_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea99e76-6575-48f8-8a37-f8118f89dff5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Setting up constants and a small demonstration configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ab7fb6-8eba-457e-8ffd-381a2753e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for demonstration\n",
    "CSIZE = 128  # Reduced size for faster processing\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "BINS = 10\n",
    "USE_SOFT_LABELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1282070-bb9e-4d32-9b3b-23be3631e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training configuration\n",
    "config_path = Path('../configs/training_config.json').expanduser().resolve()\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca069d7a-5c6d-42ca-afbd-472a8198d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepMRI@demo\n",
      "/Users/gathanasiou/oxcitas/UKB-gen-clocks/DL-apporach/configs/dataset_configs\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Extract configuration parameters\n",
    "PROJECT_NAME = config['project_name']\n",
    "OUTPUT_DIRECTORY = Path(config['output_directory']).expanduser().resolve()\n",
    "USE_SOFT_LABELS = config['use_soft_labels']\n",
    "print(PROJECT_NAME) \n",
    "print(OUTPUT_DIRECTORY) \n",
    "print(USE_SOFT_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c6276-5655-4210-8ce9-96fdffd1a34d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050d5af-8059-4c64-bce1-709ac018483c",
   "metadata": {},
   "source": [
    "## Back up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5de1fbb-a61d-4037-8097-1758e2a95318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "    \"\"\"\n",
    "    Prepare the dataset by appending file extensions to scan IDs.\n",
    "    \"\"\"\n",
    "    dataset.scanIDs = [scan + '.mgz' for scan in dataset.scanIDs]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a1ba4f-fa35-4cdf-8a4b-bdc9a1fab764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_batches(X_train, y_train, X_test, y_test, batch_size, soft_labels=False):\n",
    "    \"\"\"\n",
    "    Create TensorFlow data batches for training and testing.\n",
    "    \"\"\"\n",
    "    if soft_labels:\n",
    "        train_labels = np.array([item for item in y_train])\n",
    "        test_labels = np.array([item for item in y_test])\n",
    "    else:\n",
    "        train_labels = np.array(y_train)\n",
    "        test_labels = np.array(y_test)\n",
    "\n",
    "    train_scans = tf.data.Dataset.from_tensor_slices((X_train, train_labels))\n",
    "    test_scans = tf.data.Dataset.from_tensor_slices((X_test, test_labels))\n",
    "\n",
    "    train_scans = train_scans.map(load_scan, num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
    "    test_scans = test_scans.map(load_scan, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    BUFFER_SIZE = 500\n",
    "    train_batches = (\n",
    "        train_scans\n",
    "        .cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .filter(lambda x, y: tf.shape(x)[0] > 1)  # Filter out small batches\n",
    "        .repeat()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    test_batches = test_scans.batch(batch_size).filter(lambda x, y: tf.shape(x)[0] > 1)  # Filter out small batches\n",
    "\n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9979b639-117f-4192-baea-04e314d7da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_real_labels(model, learning_rate):\n",
    "    \"\"\"\n",
    "    Compile the model for real labels with the custom AWD loss function and Adam optimizer.\n",
    "    \"\"\"\n",
    "    awdLoss = AWDLoss(num_bins=C.BINS, initial_alpha=C.ALPHA, initial_beta=C.BETA)\n",
    "    \n",
    "    def awd_loss(y_true, y_pred):\n",
    "        return awdLoss(y_true, y_pred)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=awd_loss,\n",
    "                  metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e15371-f5a7-4787-ba97-0f3f065cffdb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c8f0d-7963-41c2-a16c-81e0335e1a42",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Loading real data based on the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ae8885-3033-48a6-9044-89025435417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Target path:  ../configs/targets/real_targets.json\n"
     ]
    }
   ],
   "source": [
    "# Load dataset configuration\n",
    "DATASET_CONFIG_PATH = OUTPUT_DIRECTORY / ('sample_config_soft.json' if USE_SOFT_LABELS else 'sample_config_real.json')\n",
    "data = Dataset.from_file(DATASET_CONFIG_PATH)\n",
    "data = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce742089-8a23-4abf-abb3-bec8ace6bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 9\n",
      "Training subjects: 6, Test subjects: 3\n",
      "Train Scans: 7, Train Labels: 7\n",
      "Test Scans: 3, Test Labels: 3\n",
      "Formatted Train Labels Shape: (7,), Test Labels Shape: (3,)\n",
      "Training fold 1...\n",
      "Number of cases in X_train: 7\n",
      "Number of cases in X_test: 3\n"
     ]
    }
   ],
   "source": [
    "# Prepare train-test splits\n",
    "for fold_num, (X_train, X_test, y_train, y_test) in enumerate(data.train_test_subjID_iterator(), 1):\n",
    "    print(f\"Training fold {fold_num}...\")\n",
    "    print(f\"Number of cases in X_train: {len(X_train)}\")\n",
    "    print(f\"Number of cases in X_test: {len(X_test)}\")\n",
    "\n",
    "    train_batches, test_batches = create_data_batches(X_train, y_train, X_test, y_test, BATCH_SIZE, soft_labels=USE_SOFT_LABELS)\n",
    "    break  # Only load the first fold for this demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef31c9-36a7-4bcf-8070-9658af3b94f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c119aaf-c05f-4b0e-8b18-fb9979bfa759",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Training the model on a small dataset for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fde343d1-453b-49fe-8254-e5577f51fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 9\n",
      "Training subjects: 6, Test subjects: 3\n",
      "Train Scans: 7, Train Labels: 7\n",
      "Test Scans: 3, Test Labels: 3\n",
      "Formatted Train Labels Shape: (7,), Test Labels Shape: (3,)\n",
      "Training fold 1...\n",
      "Number of cases in X_train: 7\n",
      "Number of cases in X_test: 3\n",
      "Regression case initialized...\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - loss: 158.4722 - mean_absolute_error: 81.2461 - mean_squared_error: 6651.5303\n",
      "Epoch 1: val_loss improved from inf to 102.71008, saving model to ../data/model_output/fold_1/best_weights_fold_1.weights.h5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 158.4722 - mean_absolute_error: 81.2461 - mean_squared_error: 6651.5303 - val_loss: 102.7101 - val_mean_absolute_error: 67.4598 - val_mean_squared_error: 4559.9312 - learning_rate: 0.0100\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - loss: 115.6207 - mean_absolute_error: 80.0908 - mean_squared_error: 6476.3164\n",
      "Epoch 2: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 115.6207 - mean_absolute_error: 80.0908 - mean_squared_error: 6476.3164 - val_loss: 2520.0310 - val_mean_absolute_error: 1672.2985 - val_mean_squared_error: 2797524.7500 - learning_rate: 0.0100\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - loss: 150.1960 - mean_absolute_error: 77.6756 - mean_squared_error: 6083.5459\n",
      "Epoch 3: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 150.1960 - mean_absolute_error: 77.6756 - mean_squared_error: 6083.5459 - val_loss: 5071.2275 - val_mean_absolute_error: 3365.8252 - val_mean_squared_error: 11332441.0000 - learning_rate: 0.0100\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - loss: 109.3614 - mean_absolute_error: 75.2960 - mean_squared_error: 5729.0078\n",
      "Epoch 4: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 109.3614 - mean_absolute_error: 75.2960 - mean_squared_error: 5729.0078 - val_loss: 5271.3037 - val_mean_absolute_error: 3501.5059 - val_mean_squared_error: 12263573.0000 - learning_rate: 0.0100\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - loss: 136.8191 - mean_absolute_error: 70.1211 - mean_squared_error: 4946.2568\n",
      "Epoch 5: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 136.8191 - mean_absolute_error: 70.1211 - mean_squared_error: 4946.2568 - val_loss: 5654.9136 - val_mean_absolute_error: 3753.6702 - val_mean_squared_error: 14094312.0000 - learning_rate: 0.0100\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - loss: 119.3922 - mean_absolute_error: 81.0958 - mean_squared_error: 6589.2612\n",
      "Epoch 6: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 119.3922 - mean_absolute_error: 81.0958 - mean_squared_error: 6589.2612 - val_loss: 6128.8169 - val_mean_absolute_error: 4065.8518 - val_mean_squared_error: 16537352.0000 - learning_rate: 0.0100\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - loss: 141.1139 - mean_absolute_error: 72.3356 - mean_squared_error: 5274.7295\n",
      "Epoch 7: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 141.1139 - mean_absolute_error: 72.3356 - mean_squared_error: 5274.7295 - val_loss: 5353.7627 - val_mean_absolute_error: 3552.9524 - val_mean_squared_error: 12627959.0000 - learning_rate: 0.0100\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - loss: 106.1401 - mean_absolute_error: 73.8619 - mean_squared_error: 5519.3599\n",
      "Epoch 8: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 106.1401 - mean_absolute_error: 73.8619 - mean_squared_error: 5519.3599 - val_loss: 4616.5903 - val_mean_absolute_error: 3064.5676 - val_mean_squared_error: 9394792.0000 - learning_rate: 0.0100\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - loss: 139.4909 - mean_absolute_error: 71.3215 - mean_squared_error: 5114.6465\n",
      "Epoch 9: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 139.4909 - mean_absolute_error: 71.3215 - mean_squared_error: 5114.6465 - val_loss: 3605.6101 - val_mean_absolute_error: 2395.3181 - val_mean_squared_error: 5739260.0000 - learning_rate: 0.0100\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - loss: 100.8252 - mean_absolute_error: 70.8491 - mean_squared_error: 5111.4614\n",
      "Epoch 10: val_loss did not improve from 102.71008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 100.8252 - mean_absolute_error: 70.8491 - mean_squared_error: 5111.4614 - val_loss: 2809.8357 - val_mean_absolute_error: 1868.0898 - val_mean_squared_error: 3490679.2500 - learning_rate: 0.0100\n"
     ]
    }
   ],
   "source": [
    "for fold_num, (X_train, X_test, y_train, y_test) in enumerate(data.train_test_subjID_iterator(), 1):\n",
    "    print(f\"Training fold {fold_num}...\")\n",
    "    print(f\"Number of cases in X_train: {len(X_train)}\")\n",
    "    print(f\"Number of cases in X_test: {len(X_test)}\")\n",
    "\n",
    "    # y_train_bins, train_bins = discretize_ages(y_train)\n",
    "    # y_test_bins, test_bins = discretize_ages(y_test)\n",
    "\n",
    "    model_pick = C.RESNET\n",
    "    if USE_SOFT_LABELS:\n",
    "        print(\"Soft labeling case initialized...\")\n",
    "        if model_pick:\n",
    "            print(\"Using ResNet model...\")\n",
    "            model = resnet18(input_shape=(C.CSIZE, C.CSIZE, C.CSIZE, 1),\n",
    "                     num_classes=int((C.HGH_BIN - C.LOW_BIN) / C.STEP_BIN), \n",
    "                     channel_size=[64, 64, 128, 256, 512], \n",
    "                     dropout=True)\n",
    "        else:\n",
    "            print(\"Using SFCN model...\")\n",
    "            model = SFCN(input_shape=(C.CSIZE, C.CSIZE, C.CSIZE, 1), \n",
    "                        channels=[32, 64, 128, 256, 256, 64], \n",
    "                        output_dim=int((C.HGH_BIN - C.LOW_BIN) / C.STEP_BIN), \n",
    "                        use_dropout=True, \n",
    "                        csize=C.CSIZE).build_model()\n",
    "        model = compile_model_soft_labels(model, C.LEARNING_RATE)\n",
    "        train_batches, test_batches = create_data_batches(X_train, y_train, X_test, y_test, C.BATCH_SIZE, soft_labels=True)\n",
    "    else:\n",
    "        print(\"Regression case initialized...\")\n",
    "        model = Custom3DCNN(input_shape=(C.CSIZE, C.CSIZE, C.CSIZE, 1),\n",
    "                            depth=C.DEPTH, \n",
    "                            initial_filters=C.INITIAL_FILTERS, \n",
    "                            l2_strength=C.L2_STRENGTH).build_model()\n",
    "        model = compile_model_real_labels(model, C.LEARNING_RATE)\n",
    "        train_batches, test_batches = create_data_batches(X_train, y_train, X_test, y_test, C.BATCH_SIZE)\n",
    "\n",
    "    STEPS_PER_EPOCH = len(X_train) // C.BATCH_SIZE\n",
    "    checkpoint_dir_fold = os.path.join('../data/', f\"model_output/fold_{fold_num}\")\n",
    "    os.makedirs(checkpoint_dir_fold, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir_fold, f\"best_weights_fold_{fold_num}.weights.h5\")\n",
    "\n",
    "    if C.LOAD_WEIGHTS and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading weights from {checkpoint_path}...\")\n",
    "        model.load_weights(checkpoint_path)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                 save_weights_only=True,\n",
    "                                 monitor='val_loss',\n",
    "                                 mode='min',\n",
    "                                 save_best_only=True,\n",
    "                                 verbose=1)\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "    log_file_path = os.path.join('../data/', 'logs', f'metrics_log_fold_{fold_num}.txt')\n",
    "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "    \n",
    "    save_subject_ids(checkpoint_dir_fold, fold_num, X_train, X_test)\n",
    "\n",
    "    callbacks = [checkpoint, lr_scheduler]\n",
    "\n",
    "    history = model.fit(train_batches,\n",
    "                        epochs=C.EPOCHS,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        validation_data=test_batches,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340b87c-947f-4a8f-8fbc-4885956dd305",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62cbae-7501-4cb3-83f1-8a08ea034ef3",
   "metadata": {},
   "source": [
    "## Evaluation and Predictions\n",
    "Evaluate the trained model and demonstrate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7181753-905d-44af-8fb3-5ae4df2366f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ../data/model_output/fold_1/best_weights_fold_1.h5 to compute validation preds.\n"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "# checkpoint_path = checkpoint_path[:-11] + '.h5'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading weights from {checkpoint_path} to compute validation preds.\")\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3126617-bac8-4b5a-8da5-18d4d94653da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 34.8778 - mean_absolute_error: 23.6640 - mean_squared_error: 563.5627\n",
      "Evaluation Results - Loss: 34.88, MAE: 23.66, MSE: 563.56\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mae, mse = model.evaluate(test_batches)\n",
    "print(f\"Evaluation Results - Loss: {loss:.2f}, MAE: {mae:.2f}, MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7cf932d-3501-4785-aae7-994be02c9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step\n",
      "Sample Predictions:\n",
      "[61.01092]\n",
      "[58.971287]\n",
      "[61.025806]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions = model.predict(test_batches)\n",
    "print(\"Sample Predictions:\")\n",
    "for pred in predictions[:5]:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5439ec1-8580-4799-9fa2-2a06aeb9a023",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af9b1f-810d-4ce4-a4f4-28072022ef87",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates a simplified training pipeline using a limited amount of data. Adapt this template with real data and models for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28088933-2fb0-4701-b96f-0ce1e8617a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
